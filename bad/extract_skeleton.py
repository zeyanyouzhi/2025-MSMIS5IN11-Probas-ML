import cv2
import mediapipe as mp
import pandas as pd
import numpy as np
import os

# ====== ä½ è¦æ”¹çš„ä¸‰è¡Œ ======
VIDEO_PATH = r"D:\GNN_BAD\huizi.mp4"
CSV_PATH   = r"D:\GNN_BAD\impact.csv"  # ç¬¬ä¸€åˆ—æ˜¯å¸§å·
OUTPUT_NPY = r"D:\GNN_BAD\skeleton_ld_lcw.npy"
WINDOW = 16    # t0-16 ~ t0ï¼Œä¸€å…± 17 å¸§ï¼Œç¨åæˆªåˆ° 16
# =========================

# 1) è¯» impact.csv
df = pd.read_csv(CSV_PATH, encoding="utf-8", engine="python")
df.columns = df.columns.str.strip().str.replace("\ufeff", "")
impact_frames = df.iloc[:, 0].astype(int).tolist()
print("å‡»çƒå¸§åˆ—è¡¨:", impact_frames)

# 2) æ‰“å¼€è§†é¢‘
cap = cv2.VideoCapture(VIDEO_PATH)
if not cap.isOpened():
    raise RuntimeError("âŒ æ— æ³•æ‰“å¼€è§†é¢‘ï¼Œè¯·æ£€æŸ¥ VIDEO_PATH æ˜¯å¦æ­£ç¡®ï¼ˆè·¯å¾„ã€ä¸­æ–‡åç­‰ï¼‰ã€‚")

total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
fps = cap.get(cv2.CAP_PROP_FPS)
w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

print(f"æ€»å¸§æ•°: {total_frames}, FPS: {fps}, åˆ†è¾¨ç‡: {w}x{h}")

# 3) åˆå§‹åŒ– MediaPipe Pose
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(static_image_mode=False,
                    model_complexity=1,
                    enable_segmentation=False,
                    min_detection_confidence=0.5,
                    min_tracking_confidence=0.5)

# æˆ‘ä»¬è¦ç”Ÿæˆçš„æ•°æ®: (N, T, V, C)
samples = []
valid_t0 = []

for idx, t0 in enumerate(impact_frames):
    if t0 < 0 or t0 >= total_frames:
        print(f"âš  è·³è¿‡éæ³•å¸§å· {t0}")
        continue

    start = max(0, t0 - WINDOW)
    end = t0
    frames_skel = []

    print(f"\n=== å¤„ç†ç¬¬ {idx} ä¸ªå‡»çƒ: t0={t0}, åŒºé—´ {start}~{end} ===")

    for f in range(start, end + 1):
        cap.set(cv2.CAP_PROP_POS_FRAMES, f)
        ret, frame = cap.read()
        if not ret:
            print(f"âš  å¸§ {f} è¯»å–å¤±è´¥ï¼Œä¸­æ­¢è¯¥æ ·æœ¬")
            frames_skel = []
            break

        # BGR â†’ RGB
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        result = pose.process(rgb)

        if not result.pose_landmarks:
            # æ²¡æ£€æµ‹åˆ°äººï¼Œå¡« 0
            joints = np.zeros((33, 2), dtype=np.float32)
        else:
            lm = result.pose_landmarks.landmark
            joints = np.array([[p.x, p.y] for p in lm], dtype=np.float32)  # 33x2ï¼Œå½’ä¸€åŒ–åˆ° [0,1]

        frames_skel.append(joints)

    if not frames_skel:
        continue

    seq = np.stack(frames_skel, axis=0)  # (T', 33, 2)

    # å¦‚æœé•¿åº¦ä¸æ˜¯ 16ï¼Œå¯ä»¥ç»Ÿä¸€åˆ° 16ï¼ˆæ¯”å¦‚åˆ æ‰ç¬¬ä¸€å¸§ï¼‰
    if seq.shape[0] > 16:
        seq = seq[-16:, :, :]
    elif seq.shape[0] < 16:
        # ä¸è¶³å°±é‡å¤æœ€åä¸€å¸§è¡¥é½
        last = seq[-1:, :, :]
        pad = np.repeat(last, 16 - seq.shape[0], axis=0)
        seq = np.concatenate([seq, pad], axis=0)

    samples.append(seq)
    valid_t0.append(t0)
    print(f"âœ… æ ·æœ¬é•¿åº¦: {seq.shape[0]} å¸§")

pose.close()
cap.release()

if not samples:
    raise RuntimeError("âŒ æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½• skeleton æ ·æœ¬ï¼Œè¯·æ£€æŸ¥å‡»çƒå¸§å’Œè§†é¢‘ã€‚")

X = np.stack(samples, axis=0)  # (N, 16, 33, 2)
print("æœ€ç»ˆæ•°ç»„å½¢çŠ¶:", X.shape)

np.save(OUTPUT_NPY, X)
print("ğŸ‰ å·²ä¿å­˜åˆ°:", OUTPUT_NPY)
print("å¯¹åº”çš„å‡»çƒå¸§åˆ—è¡¨:", valid_t0)
